{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import inspect\n",
    "from typing import (\n",
    "    Any,\n",
    "    Callable,\n",
    "    Literal,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Type,\n",
    "    TypeVar,\n",
    "    Union,\n",
    "    cast,\n",
    "    get_type_hints,\n",
    ")\n",
    "\n",
    "from langchain_core.language_models import (\n",
    "    BaseChatModel,\n",
    "    LanguageModelInput,\n",
    "    LanguageModelLike,\n",
    ")\n",
    "from langchain_core.messages import AIMessage, BaseMessage, SystemMessage, ToolMessage\n",
    "from langchain_core.runnables import (\n",
    "    Runnable,\n",
    "    RunnableBinding,\n",
    "    RunnableConfig,\n",
    "    RunnableSequence,\n",
    ")\n",
    "from langchain_core.tools import BaseTool\n",
    "from pydantic import BaseModel\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "from langgraph.errors import ErrorCode, create_error_message\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.managed import IsLastStep, RemainingSteps\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.types import Checkpointer, Send\n",
    "from langgraph.utils.runnable import RunnableCallable\n",
    "\n",
    "StructuredResponse = Union[dict, BaseModel]\n",
    "StructuredResponseSchema = Union[dict, type[BaseModel]]\n",
    "F = TypeVar(\"F\", bound=Callable[..., Any])\n",
    "\n",
    "\n",
    "# We create the AgentState that we will pass around\n",
    "# This simply involves a list of messages\n",
    "# We want steps to return messages to append to the list\n",
    "# So we annotate the messages attribute with `add_messages` reducer\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of the agent.\"\"\"\n",
    "\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "    is_last_step: IsLastStep\n",
    "\n",
    "    remaining_steps: RemainingSteps\n",
    "\n",
    "\n",
    "class AgentStatePydantic(BaseModel):\n",
    "    \"\"\"The state of the agent.\"\"\"\n",
    "\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "    remaining_steps: RemainingSteps = 25\n",
    "\n",
    "\n",
    "class AgentStateWithStructuredResponse(AgentState):\n",
    "    \"\"\"The state of the agent with a structured response.\"\"\"\n",
    "\n",
    "    structured_response: StructuredResponse\n",
    "\n",
    "\n",
    "class AgentStateWithStructuredResponsePydantic(AgentStatePydantic):\n",
    "    \"\"\"The state of the agent with a structured response.\"\"\"\n",
    "\n",
    "    structured_response: StructuredResponse\n",
    "\n",
    "\n",
    "StateSchema = TypeVar(\"StateSchema\", bound=Union[AgentState, AgentStatePydantic])\n",
    "StateSchemaType = Type[StateSchema]\n",
    "\n",
    "PROMPT_RUNNABLE_NAME = \"Prompt\"\n",
    "\n",
    "Prompt = Union[\n",
    "    SystemMessage,\n",
    "    str,\n",
    "    Callable[[StateSchema], LanguageModelInput],\n",
    "    Runnable[StateSchema, LanguageModelInput],\n",
    "]\n",
    "\n",
    "\n",
    "def _get_state_value(state: StateSchema, key: str, default: Any = None) -> Any:\n",
    "    return (\n",
    "        state.get(key, default)\n",
    "        if isinstance(state, dict)\n",
    "        else getattr(state, key, default)\n",
    "    )\n",
    "\n",
    "\n",
    "def _get_prompt_runnable(prompt: Optional[Prompt]) -> Runnable:\n",
    "    prompt_runnable: Runnable\n",
    "    if prompt is None:\n",
    "        prompt_runnable = RunnableCallable(\n",
    "            lambda state: _get_state_value(state, \"messages\"), name=PROMPT_RUNNABLE_NAME\n",
    "        )\n",
    "    elif isinstance(prompt, str):\n",
    "        _system_message: BaseMessage = SystemMessage(content=prompt)\n",
    "        prompt_runnable = RunnableCallable(\n",
    "            lambda state: [_system_message] + _get_state_value(state, \"messages\"),\n",
    "            name=PROMPT_RUNNABLE_NAME,\n",
    "        )\n",
    "    elif isinstance(prompt, SystemMessage):\n",
    "        prompt_runnable = RunnableCallable(\n",
    "            lambda state: [prompt] + _get_state_value(state, \"messages\"),\n",
    "            name=PROMPT_RUNNABLE_NAME,\n",
    "        )\n",
    "    elif inspect.iscoroutinefunction(prompt):\n",
    "        prompt_runnable = RunnableCallable(\n",
    "            None,\n",
    "            prompt,\n",
    "            name=PROMPT_RUNNABLE_NAME,\n",
    "        )\n",
    "    elif callable(prompt):\n",
    "        prompt_runnable = RunnableCallable(\n",
    "            prompt,\n",
    "            name=PROMPT_RUNNABLE_NAME,\n",
    "        )\n",
    "    elif isinstance(prompt, Runnable):\n",
    "        prompt_runnable = prompt\n",
    "    else:\n",
    "        raise ValueError(f\"Got unexpected type for `prompt`: {type(prompt)}\")\n",
    "\n",
    "    return prompt_runnable\n",
    "\n",
    "\n",
    "def _convert_modifier_to_prompt(func: F) -> F:\n",
    "    \"\"\"Decorator that converts state_modifier kwarg to prompt kwarg.\"\"\"\n",
    "\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args: Any, **kwargs: Any) -> Any:\n",
    "        prompt = kwargs.get(\"prompt\")\n",
    "        state_modifier = kwargs.pop(\"state_modifier\", None)\n",
    "        if sum(p is not None for p in (prompt, state_modifier)) > 1:\n",
    "            raise ValueError(\n",
    "                \"Expected only one of (prompt, state_modifier), got multiple values\"\n",
    "            )\n",
    "\n",
    "        if state_modifier is not None:\n",
    "            prompt = state_modifier\n",
    "\n",
    "        kwargs[\"prompt\"] = prompt\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return cast(F, wrapper)\n",
    "\n",
    "\n",
    "def _should_bind_tools(model: LanguageModelLike, tools: Sequence[BaseTool]) -> bool:\n",
    "    if isinstance(model, RunnableSequence):\n",
    "        model = next(\n",
    "            (\n",
    "                step\n",
    "                for step in model.steps\n",
    "                if isinstance(step, (RunnableBinding, BaseChatModel))\n",
    "            ),\n",
    "            model,\n",
    "        )\n",
    "\n",
    "    if not isinstance(model, RunnableBinding):\n",
    "        return True\n",
    "\n",
    "    if \"tools\" not in model.kwargs:\n",
    "        return True\n",
    "\n",
    "    bound_tools = model.kwargs[\"tools\"]\n",
    "    if len(tools) != len(bound_tools):\n",
    "        raise ValueError(\n",
    "            \"Number of tools in the model.bind_tools() and tools passed to create_react_agent must match\"\n",
    "        )\n",
    "\n",
    "    tool_names = set(tool.name for tool in tools)\n",
    "    bound_tool_names = set()\n",
    "    for bound_tool in bound_tools:\n",
    "        # OpenAI-style tool\n",
    "        if bound_tool.get(\"type\") == \"function\":\n",
    "            bound_tool_name = bound_tool[\"function\"][\"name\"]\n",
    "        # Anthropic-style tool\n",
    "        elif bound_tool.get(\"name\"):\n",
    "            bound_tool_name = bound_tool[\"name\"]\n",
    "        else:\n",
    "            # unknown tool type so we'll ignore it\n",
    "            continue\n",
    "\n",
    "        bound_tool_names.add(bound_tool_name)\n",
    "\n",
    "    if missing_tools := tool_names - bound_tool_names:\n",
    "        raise ValueError(f\"Missing tools '{missing_tools}' in the model.bind_tools()\")\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def _get_model(model: LanguageModelLike) -> BaseChatModel:\n",
    "    \"\"\"Get the underlying model from a RunnableBinding or return the model itself.\"\"\"\n",
    "    if isinstance(model, RunnableSequence):\n",
    "        model = next(\n",
    "            (\n",
    "                step\n",
    "                for step in model.steps\n",
    "                if isinstance(step, (RunnableBinding, BaseChatModel))\n",
    "            ),\n",
    "            model,\n",
    "        )\n",
    "\n",
    "    if isinstance(model, RunnableBinding):\n",
    "        model = model.bound\n",
    "\n",
    "    if not isinstance(model, BaseChatModel):\n",
    "        raise TypeError(\n",
    "            f\"Expected `model` to be a ChatModel or RunnableBinding (e.g. model.bind_tools(...)), got {type(model)}\"\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def _validate_chat_history(\n",
    "    messages: Sequence[BaseMessage],\n",
    ") -> None:\n",
    "    \"\"\"Validate that all tool calls in AIMessages have a corresponding ToolMessage.\"\"\"\n",
    "    all_tool_calls = [\n",
    "        tool_call\n",
    "        for message in messages\n",
    "        if isinstance(message, AIMessage)\n",
    "        for tool_call in message.tool_calls\n",
    "    ]\n",
    "    tool_call_ids_with_results = {\n",
    "        message.tool_call_id for message in messages if isinstance(message, ToolMessage)\n",
    "    }\n",
    "    tool_calls_without_results = [\n",
    "        tool_call\n",
    "        for tool_call in all_tool_calls\n",
    "        if tool_call[\"id\"] not in tool_call_ids_with_results\n",
    "    ]\n",
    "    if not tool_calls_without_results:\n",
    "        return\n",
    "\n",
    "    error_message = create_error_message(\n",
    "        message=\"Found AIMessages with tool_calls that do not have a corresponding ToolMessage. \"\n",
    "        f\"Here are the first few of those tool calls: {tool_calls_without_results[:3]}.\\n\\n\"\n",
    "        \"Every tool call (LLM requesting to call a tool) in the message history MUST have a corresponding ToolMessage \"\n",
    "        \"(result of a tool invocation to return to the LLM) - this is required by most LLM providers.\",\n",
    "        error_code=ErrorCode.INVALID_CHAT_HISTORY,\n",
    "    )\n",
    "    raise ValueError(error_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Membuat Agent Organizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Any\n",
    "from langchain_core.messages import ToolMessage, AIMessage  # pastikan impor ini sesuai struktur proyekmu\n",
    "\n",
    "\n",
    "class BaseFallbackToolCalling(ABC):\n",
    "    \"\"\"Abstract base class for handling fallback tool logic.\"\"\"\n",
    "\n",
    "    tool_map: Dict[str, str] = {}\n",
    "\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def checker(\n",
    "        cls,\n",
    "        tool_message: ToolMessage\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether a fallback tool should be triggered based on the tool message.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def tool_call(\n",
    "        cls,\n",
    "        prev_tool_call: ToolMessage,\n",
    "        name: Optional[str] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Create a new tool call message as a fallback attempt.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import json\n",
    "\n",
    "\n",
    "class FallbackToolCalling(BaseFallbackToolCalling):\n",
    "    tool_map = {\n",
    "        \"text2cypher_retriever\": \"vector_cypher_retriever\"\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def checker(\n",
    "        cls,\n",
    "        tool_message: ToolMessage\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether a fallback tool should be triggered based on the tool message.\n",
    "        \"\"\"\n",
    "        for availabel_tool_name in cls.tool_map.keys():\n",
    "            if tool_message.name in availabel_tool_name:\n",
    "                should_call_alternate_tool = not tool_message.artifact.get(\"is_context_fetched\", False)\n",
    "                return should_call_alternate_tool\n",
    "        return False\n",
    "\n",
    "    @classmethod\n",
    "    def tool_call(\n",
    "        cls,\n",
    "        prev_tool_call: ToolMessage,\n",
    "        name: Optional[str] = None\n",
    "    ) -> AIMessage:\n",
    "        \"\"\"\n",
    "        Create a new tool call message as a fallback attempt.\n",
    "        \"\"\"\n",
    "        for availabel_tool_name in cls.tool_map.keys():\n",
    "            if prev_tool_call.tool_calls[0]['name'] in availabel_tool_name:\n",
    "                alternate_tool = cls.tool_map[availabel_tool_name]\n",
    "\n",
    "        return AIMessage(\n",
    "            content=(\n",
    "                \"Tidak dapat menemukan data yang sesuai dengan permintaan query: \"\n",
    "                f\"{prev_tool_call.additional_kwargs['function_call']['arguments']} dengan \"\n",
    "                f\"menggunakan tool {prev_tool_call.tool_calls[0]['name']}. Mencoba ulang \"\n",
    "                f\"pencarian data dengan menggunakan tool alternatif: {alternate_tool}\"\n",
    "            ),\n",
    "            additional_kwargs={\n",
    "                \"function_call\": {\n",
    "                    \"name\": alternate_tool,\n",
    "                    \"arguments\": prev_tool_call.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "                }\n",
    "            },\n",
    "            response_metadata={},\n",
    "            type=prev_tool_call.type,\n",
    "            name=name,\n",
    "            id=f\"run-{uuid.uuid4()}-0\",\n",
    "            example=prev_tool_call.example,\n",
    "            tool_calls=[{\n",
    "                \"name\": alternate_tool,\n",
    "                # Di langchain ada schema parser, coba itu buat parse str ini menjadi dict, dan bukan pakai json.loads()\n",
    "                \"args\": json.loads(prev_tool_call.additional_kwargs[\"function_call\"][\"arguments\"]),\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"type\": \"tool_call\"\n",
    "            }],\n",
    "            invalid_tool_calls=prev_tool_call.invalid_tool_calls,\n",
    "            usage_metadata = {\n",
    "                'input_tokens': 0,\n",
    "                'output_tokens': 0,\n",
    "                'total_tokens': 0,\n",
    "                # Bagian di bawah ini ternyata di hasil ChatOllama tidak ada\n",
    "                'input_token_details': {\n",
    "                    'cache_read': 0\n",
    "                }\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent(\n",
    "    model: BaseChatModel,\n",
    "    tools: Sequence[Union[BaseTool, Callable]],\n",
    "    *,\n",
    "    prompt: Optional[Prompt] = None,\n",
    "    name: Optional[str] = None,\n",
    "    fallback_tool_calling: Optional[Type[BaseFallbackToolCalling]] = None,\n",
    "):\n",
    "    if isinstance(tools, ToolNode):\n",
    "        tool_classes = list(tools.tools_by_name.values())\n",
    "    else:\n",
    "        tool_node = ToolNode(tools)\n",
    "        # get the tool functions wrapped in a tool class from the ToolNode\n",
    "        tool_classes = list(tool_node.tools_by_name.values())\n",
    "\n",
    "    tool_calling_enabled = len(tool_classes) > 0\n",
    "\n",
    "    if _should_bind_tools(model, tool_classes) and tool_calling_enabled:\n",
    "        model = cast(BaseChatModel, model).bind_tools(tool_classes)\n",
    "\n",
    "    model_runnable = _get_prompt_runnable(prompt) | model\n",
    "    \n",
    "    def _are_more_steps_needed(state: StateSchema, response: BaseMessage) -> bool:\n",
    "        has_tool_calls = isinstance(response, AIMessage) and response.tool_calls\n",
    "        remaining_steps = _get_state_value(state, \"remaining_steps\", None)\n",
    "        is_last_step = _get_state_value(state, \"is_last_step\", False)\n",
    "        return (\n",
    "            (remaining_steps is None and is_last_step and has_tool_calls)\n",
    "            or (remaining_steps is not None and remaining_steps < 2 and has_tool_calls)\n",
    "        )\n",
    "\n",
    "    # Define the function that calls the model\n",
    "    def call_model(state: StateSchema, config: RunnableConfig) -> StateSchema:\n",
    "        messages = _get_state_value(state, \"messages\")\n",
    "        _validate_chat_history(messages)\n",
    "\n",
    "        ######################## BUATANKU ##########################\n",
    "        # Cek jika tool sebelumnya gagal mendapatkan data\n",
    "        last_message = messages[-1]\n",
    "        if fallback_tool_calling is not None and isinstance(last_message, ToolMessage):\n",
    "            if fallback_tool_calling.checker(last_message):\n",
    "                print(\"FOLLBACK\")\n",
    "                tool_call_message = fallback_tool_calling.tool_call(\n",
    "                    prev_tool_call=messages[-2], name=name\n",
    "                )\n",
    "                return {\n",
    "                    \"messages\": [tool_call_message],\n",
    "                    \"steps\": [name]\n",
    "                }\n",
    "        print(\"NO FOLLBACK\")\n",
    "        ############################################################\n",
    "        # TODO: BARU SAMPAI SINI, BELUM TESTING\n",
    "        \n",
    "        response = cast(AIMessage, model_runnable.invoke(state, config))\n",
    "        # add agent name to the AIMessage\n",
    "        response.name = name\n",
    "\n",
    "        if _are_more_steps_needed(state, response):\n",
    "            return {\n",
    "                \"messages\": [\n",
    "                    AIMessage(\n",
    "                        id=response.id,\n",
    "                        content=(\n",
    "                            \"Maaf, perlu langkah lebih lanjut untuk memproses \"\n",
    "                            \"permintaan ini.\"\n",
    "                        ),\n",
    "                    )\n",
    "                ],\n",
    "                \"steps\": [name]\n",
    "            }\n",
    "        # We return a list, because this will get added to the existing list\n",
    "        return {\n",
    "            \"messages\": [response],\n",
    "            \"steps\": [name]\n",
    "        }\n",
    "\n",
    "    return RunnableCallable(call_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Membuat Agent Perangkum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bisa aku modifikasi sehingga menjadi Node Perangkum\n",
    "# Ini bisa di skip dulu, jika agent di atas sudah jadi,\n",
    "# maka lanjut ke workflow dulu saja, hingga testing.\n",
    "# Kerjakan ini sesudah yang lainnya bisa\n",
    "\n",
    "def generate_structured_response(\n",
    "    state: StateSchema, config: RunnableConfig\n",
    ") -> StateSchema:\n",
    "    # NOTE: we exclude the last message because there is enough information\n",
    "    # for the LLM to generate the structured response\n",
    "    messages = _get_state_value(state, \"messages\")[:-1]\n",
    "    structured_response_schema = response_format\n",
    "    if isinstance(response_format, tuple):\n",
    "        system_prompt, structured_response_schema = response_format\n",
    "        messages = [SystemMessage(content=system_prompt)] + list(messages)\n",
    "\n",
    "    model_with_structured_output = _get_model(model).with_structured_output(\n",
    "        cast(StructuredResponseSchema, structured_response_schema)\n",
    "    )\n",
    "    response = model_with_structured_output.invoke(messages, config)\n",
    "    return {\"structured_response\": response}\n",
    "\n",
    "async def agenerate_structured_response(\n",
    "    state: StateSchema, config: RunnableConfig\n",
    ") -> StateSchema:\n",
    "    # NOTE: we exclude the last message because there is enough information\n",
    "    # for the LLM to generate the structured response\n",
    "    messages = _get_state_value(state, \"messages\")[:-1]\n",
    "    structured_response_schema = response_format\n",
    "    if isinstance(response_format, tuple):\n",
    "        system_prompt, structured_response_schema = response_format\n",
    "        messages = [SystemMessage(content=system_prompt)] + list(messages)\n",
    "\n",
    "    model_with_structured_output = _get_model(model).with_structured_output(\n",
    "        cast(StructuredResponseSchema, structured_response_schema)\n",
    "    )\n",
    "    response = await model_with_structured_output.ainvoke(messages, config)\n",
    "    return {\"structured_response\": response}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Membuat Worflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_workflow(\n",
    "    model: Union[str, LanguageModelLike],\n",
    "    tools: Sequence[Union[BaseTool, Callable]],\n",
    "    *,\n",
    "    prompt: Optional[Prompt] = None,\n",
    "    response_format: Optional[\n",
    "        Union[StructuredResponseSchema, tuple[str, StructuredResponseSchema]]\n",
    "    ] = None,  # Sementara, untuk testing\n",
    "    state_schema: Optional[StateSchemaType] = None,\n",
    "    config_schema: Optional[Type[Any]] = None,\n",
    "    checkpointer: Optional[Checkpointer] = None,\n",
    "    store: Optional[BaseStore] = None,\n",
    "    name: Optional[str] = None,\n",
    "    fallback_tool_calling: Optional[Type[BaseFallbackToolCalling]] = None\n",
    "):\n",
    "    if state_schema is not None:\n",
    "        required_keys = {\"messages\", \"remaining_steps\"}\n",
    "\n",
    "        schema_keys = set(get_type_hints(state_schema))\n",
    "        if missing_keys := required_keys - set(schema_keys):\n",
    "            raise ValueError(f\"Missing required key(s) {missing_keys} in state_schema\")\n",
    "\n",
    "    if state_schema is None:\n",
    "        state_schema = AgentState  # Pakai AgentStateWithStructuredResponse jika Agent Perangkum sudah jadi\n",
    "\n",
    "    tool_node = ToolNode(tools)\n",
    "    \n",
    "    # JIKA not last_message.tool_calls MAKA ARAHKAN KE NODE PERANGKUM\n",
    "    # Define the function that determines whether to continue or not\n",
    "    def should_continue(state: StateSchema) -> Union[str, list]:\n",
    "        messages = _get_state_value(state, \"messages\")\n",
    "        last_message = messages[-1]\n",
    "        # If there is no function call, then we finish\n",
    "        if not isinstance(last_message, AIMessage) or not last_message.tool_calls:\n",
    "            return END if response_format is None else \"generate_structured_response\"\n",
    "        # Otherwise if there is, we continue\n",
    "        else:\n",
    "            # return \"tools\"\n",
    "            # NGGA TAHU KENAPA KALAU PAKAI V2 ITU MALAH ERROR\n",
    "            tool_calls = [\n",
    "                tool_node.inject_tool_args(call, state, store)  # type: ignore[arg-type]\n",
    "                for call in last_message.tool_calls\n",
    "            ]\n",
    "            return [Send(\"tools\", [tool_call]) for tool_call in tool_calls]\n",
    "    \n",
    "    # MEMBUAT WORKFLOW\n",
    "    #########################################################################################\n",
    "    \n",
    "    # Define a new graph\n",
    "    workflow = StateGraph(state_schema or AgentState, config_schema=config_schema)\n",
    "\n",
    "    # Define the two nodes we will cycle between\n",
    "    workflow.add_node(\"agent\", create_agent(\n",
    "            model=model,\n",
    "            tools=tool_node,\n",
    "            prompt=prompt,\n",
    "            name=\"agent\",\n",
    "            fallback_tool_calling=fallback_tool_calling\n",
    "        )\n",
    "    )\n",
    "    workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "    # Set the entrypoint as `agent`\n",
    "    # This means that this node is the first one called\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    \n",
    "    # We now add a conditional edge\n",
    "    workflow.add_conditional_edges(\n",
    "        # First, we define the start node. We use `agent`.\n",
    "        # This means these are the edges taken after the `agent` node is called.\n",
    "        \"agent\",\n",
    "        # Next, we pass in the function that will determine which node is called next.\n",
    "        should_continue,\n",
    "        path_map=[\"tools\", END]\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    # Finally, we compile it!\n",
    "    # This compiles it into a LangChain Runnable,\n",
    "    # meaning you can use it as you would any other runnable\n",
    "    return workflow.compile(\n",
    "        checkpointer=checkpointer,\n",
    "        store=store,\n",
    "        name=name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from src.grag.retrievers.vector_cypher.vector_cypher import create_vector_cypher_retriever_tool\n",
    "from src.grag.retrievers.text2cypher.text2cypher import create_text2cypher_retriever_tool\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URI = os.environ[\"DATABASE_HOST\"]\n",
    "DATABASE = os.environ[\"DATABASE_SMALL\"]\n",
    "USERNAME = os.environ[\"DATABASE_USERNAME\"]\n",
    "PASSWORD = os.environ[\"DATABASE_PASSWORD\"]\n",
    "DATABASE = os.environ[\"DATABASE_SMALL\"]\n",
    "\n",
    "neo4j_config = {\n",
    "    \"DATABASE_NAME\": DATABASE,\n",
    "    \"ARTICLE_VECTOR_INDEX_NAME\": os.environ[\"ARTICLE_VECTOR_INDEX_NAME\"],\n",
    "    \"ARTICLE_FULLTEXT_INDEX_NAME\": os.environ[\"ARTICLE_FULLTEXT_INDEX_NAME\"],\n",
    "    \"DEFINITION_VECTOR_INDEX_NAME\": os.environ[\"DEFINITION_VECTOR_INDEX_NAME\"],\n",
    "    \"DEFINITION_FULLTEXT_INDEX_NAME\": os.environ[\"DEFINITION_FULLTEXT_INDEX_NAME\"],\n",
    "}\n",
    "\n",
    "# Ngga jadi pakai ini karena Text2Cypher mintannya harus pakai Neo4jGraph\n",
    "# neo4j_driver = GraphDatabase.driver(uri=URI, auth=(USERNAME, PASSWORD))\n",
    "\n",
    "neo4j_graph = Neo4jGraph(\n",
    "    url=URI,\n",
    "    username=USERNAME,\n",
    "    password=PASSWORD,\n",
    "    database=DATABASE,\n",
    "    enhanced_schema=True\n",
    ")\n",
    "\n",
    "neo4j_driver = neo4j_graph._driver  # Ambil driver nya kaya gini\n",
    "\n",
    "embedder_model = HuggingFaceEmbeddings(model_name=os.environ[\"EMBEDDING_MODEL\"])\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.0,\n",
    "    api_key=os.environ[\"GOOGLE_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_cypher_retriever = create_vector_cypher_retriever_tool(\n",
    "    embedder_model=embedder_model,\n",
    "    neo4j_driver=neo4j_driver,\n",
    "    neo4j_config=neo4j_config,\n",
    "    top_k_initial_article=5,\n",
    "    total_article_limit=10,\n",
    "    total_definition_limit=5\n",
    ")\n",
    "\n",
    "text2cypher_retriever = create_text2cypher_retriever_tool(\n",
    "    neo4j_graph=neo4j_graph,\n",
    "    embedder_model=embedder_model,\n",
    "    cypher_llm=llm,\n",
    "    qa_llm=llm,\n",
    "    skip_qa_llm=True,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harus memastikan hanya memanggil 1 tool dalam sebuah pemanggilan\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an intelligent assistant that can query a legal graph database (Neo4j) using `text2cypher_retriever` or `vector_cypher_retriever`. Your goal is to accurately `Answer` user queries by utilizing some tools to fetch relevant information.\n",
    "\n",
    "### Instructions:\n",
    "1. **Understand the User Query**\n",
    "   - Carefully analyze the user's question and determine the best approach to retrieve the required information.\n",
    "\n",
    "2. **Use Available Tools**\n",
    "   - If the user asks **general questions** that can't be writed in Neo4j Cypher, use `vector_cypher_retriever`.\n",
    "   - If the user asks for **regulation structure, relationships, or anything that can be represented as Neo4j Cypher**, use `text2cypher_retriever`.\n",
    "   - Make sure to only call 1 tool in a call\n",
    "\n",
    "3. **Maintain Accuracy and Completeness**\n",
    "   - Your default language is English, but you should `Answer` the user query in the same language as the query.\n",
    "   - Always provide precise and concise `Answer` based on the retrieved data.\n",
    "   - If the retrieved data contains legal articles with subsections, structure them in a markdown list format.\n",
    "   - Ensure that your final `Answer` is well-formatted in Markdown.\n",
    "\n",
    "5. **Handle Errors Gracefully**\n",
    "   - If you dont have the `Answer`, inform the user that no relevant information was found, instead of making assumptions.\n",
    "   - If the query is ambiguous, ask for clarification before proceeding.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SYSTEM_PROMPT = \"\"\"You are an intelligent assistant that can query a legal graph database (Neo4j) using `text2cypher_retriever` or `vector_cypher_retriever`. Your goal is to accurately `Answer` user queries by utilizing some tools to fetch relevant information.\n",
    "\n",
    "# ### Instructions:\n",
    "# 1. **Understand the User Query**\n",
    "#    - Carefully analyze the user's question and determine the best approach to retrieve the required information.\n",
    "\n",
    "# 2. **Use Available Tools**\n",
    "#    - If the user asks **general questions** that can't be writed in Neo4j Cypher, use `vector_cypher_retriever`.\n",
    "#    - If the user asks for **regulation structure, relationships, or anything that can be represented as Neo4j Cypher**, use `text2cypher_retriever`.\n",
    "\n",
    "# 3. **How to Answer**\n",
    "#    - Output tool calling to retrieve relevant data.\n",
    "#    - Finally, if you know the `Answer`, then just return \"KNOW\" to user.\n",
    "\n",
    "# 5. **Handle Errors Gracefully**\n",
    "#    - If you dont have the `Answer`, inform the user that no relevant information was found, instead of making assumptions.\n",
    "#    - If the query is ambiguous, ask for clarification before proceeding.)`\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "config_1 = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "config_2 = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "workflow = create_workflow(\n",
    "    llm, [text2cypher_retriever, vector_cypher_retriever],\n",
    "    prompt=SYSTEM_PROMPT,\n",
    "    checkpointer=checkpointer,\n",
    "    fallback_tool_calling=FallbackToolCalling\n",
    ")\n",
    "\n",
    "# display(Image(workflow.get_graph().draw_mermaid_png()))\n",
    "# print(workflow.get_graph().draw_mermaid())\n",
    "print(workflow.get_graph().draw_ascii())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fallback tool calling\n",
    "\n",
    "query = \"Apa isi dari pasal 100 UU no 11 tahun 2008 apa ya?\"\n",
    "response = workflow.invoke({\"messages\": query}, config_1)\n",
    "display(response[\"messages\"])\n",
    "print(response[\"messages\"][-1].content)\n",
    "\n",
    "# for s in workflow.stream({\"messages\": query}, config_1, stream_mode=\"values\"):\n",
    "#     message = s[\"messages\"][-1]\n",
    "#     message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi tool call\n",
    "\n",
    "query = (\n",
    "    \"Isi dari pasal 28 UU no 11 tahun 2008 apa ya? \"\n",
    "    \"Kemudian carikan pasal di peraturan lain yang mana isinya mirip dengan pasal tersebut \"\n",
    "    \"(hint pakai text2cypher RELATED_TO)\"\n",
    ")\n",
    "response = workflow.invoke({\"messages\": query}, config_1)\n",
    "display(response[\"messages\"])\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_tool_call = AIMessage(content='', additional_kwargs={'function_call': {'name': 'text2cypher', 'arguments': '{\"query\": \"isi dari pasal 100 UU no 11 tahun 2008\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-b16b6f7e-1caa-46a6-aedd-6716aeafb859-0', tool_calls=[{'name': 'text2cypher', 'args': {'query': 'isi dari pasal 100 UU no 11 tahun 2008'}, 'id': '47b70b8b-22f0-4142-91f9-6859e60100b3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 393, 'output_tokens': 23, 'total_tokens': 416, 'input_token_details': {'cache_read': 0}})\n",
    "\n",
    "tool_message = ToolMessage(content=\"### **Hasil Pembuatan Kode Cypher:**\\n```cypher\\nMATCH (r:Regulation)-[:HAS_ARTICLE]->(a:Article)\\nWHERE r.type = 'UU' AND r.number = 11 AND r.year = 2008 AND a.number = '100'\\nRETURN a.text AS text\\n```\\n\\n### **Hasil Eksekusi Kode Cypher ke Database:**\\nTidak dapat menemukan data yang sesuai dengan permintaan query\", name='text2cypher', id='ad5822ca-c8a0-4798-a162-6f51e3dcd8f8', tool_call_id='47b70b8b-22f0-4142-91f9-6859e60100b3', artifact={'is_context_fetched': False, 'cypher_gen_usage_metadata': {'input_tokens': 2487, 'output_tokens': 66, 'total_tokens': 2553}, 'qa_usage_metadata': {'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}, 'run_time': 0.9400525093078613})\n",
    "\n",
    "print(FallbackToolCalling.checker(tool_message))\n",
    "print(FallbackToolCalling.tool_call(prev_tool_call))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Apa isi pasal 28 UU No. 11 Tahun 2008? Tolong tampilkan pasal aslinya, kemudian tafsirkan isinya\"\n",
    "response = workflow.invoke({\"messages\": query}, config_2)\n",
    "display(response[\"messages\"])\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Bagaimana dengan pasal setelah pasal 28 tersebut? Maksudku, carikan pasal setelah 28\"\n",
    "response = workflow.invoke({\"messages\": query}, config_2)\n",
    "display(response[\"messages\"])\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Apakah kamu tahu apa kewajiban dari penyelenggara sistem elekronik?\"\n",
    "response = workflow.invoke({\"messages\": query}, config_2)\n",
    "display(response[\"messages\"])\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Eh tadi isi pasal 28 dan 29 apa ya? Aku lupa (hint, tidak usah pakai tool calling)\"\n",
    "query = \"Eh tadi isi pasal 28 dan 29 apa ya? Tadi sudah kamu jelaskan, tapi aku lupa\"  # berhasil tanpa tool calling\n",
    "# query = \"Eh tadi isi pasal 28 dan 29 apa ya?\" # Bisa salah, tapi fallback ke vector_cypher\n",
    "response = workflow.invoke({\"messages\": query}, config_2)\n",
    "display(response[\"messages\"])\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNGGU DULU BEBERAPA DETIK SEBELUM RUN KODING INI: KARENA KENA LIMIT GOOGLE GEMINI\n",
    "\n",
    "query = \"Kalau isi pasal 100 UU no 11 tahun 2008 apa ya?\"\n",
    "response = workflow.invoke({\"messages\": query}, config_2)\n",
    "display(response[\"messages\"])\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Oalah begitu, memangnya berapa total pasal di UU tersebut?\"\n",
    "response = workflow.invoke({\"messages\": query}, config_2)\n",
    "display(response[\"messages\"])\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Apa definisi 'data pribadi pengguna' dan pasal-pasal apa saja yang menjelaskan mengenai itu?\"\n",
    "response = workflow.invoke({\"messages\": query}, config_2)\n",
    "display(response[\"messages\"])\n",
    "print(response[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
