{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRE4rGEF6-cv"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import create_react_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Y8Yoyvd6-cw"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import inspect\n",
        "from typing import (\n",
        "    Any,\n",
        "    Callable,\n",
        "    Literal,\n",
        "    Optional,\n",
        "    Sequence,\n",
        "    Type,\n",
        "    TypeVar,\n",
        "    Union,\n",
        "    cast,\n",
        "    get_type_hints,\n",
        ")\n",
        "\n",
        "from langchain_core.language_models import (\n",
        "    BaseChatModel,\n",
        "    LanguageModelInput,\n",
        "    LanguageModelLike,\n",
        ")\n",
        "from langchain_core.messages import AIMessage, BaseMessage, SystemMessage, ToolMessage\n",
        "from langchain_core.runnables import (\n",
        "    Runnable,\n",
        "    RunnableBinding,\n",
        "    RunnableConfig,\n",
        "    RunnableSequence,\n",
        ")\n",
        "from langchain_core.tools import BaseTool\n",
        "from pydantic import BaseModel\n",
        "from typing_extensions import Annotated, TypedDict\n",
        "\n",
        "from langgraph.errors import ErrorCode, create_error_message\n",
        "from langgraph.graph import END, StateGraph\n",
        "from langgraph.graph.graph import CompiledGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.managed import IsLastStep, RemainingSteps\n",
        "from langgraph.prebuilt.tool_node import ToolNode\n",
        "from langgraph.store.base import BaseStore\n",
        "from langgraph.types import Checkpointer, Send\n",
        "from langgraph.utils.runnable import RunnableCallable\n",
        "\n",
        "StructuredResponse = Union[dict, BaseModel]\n",
        "StructuredResponseSchema = Union[dict, type[BaseModel]]\n",
        "F = TypeVar(\"F\", bound=Callable[..., Any])\n",
        "\n",
        "\n",
        "# We create the AgentState that we will pass around\n",
        "# This simply involves a list of messages\n",
        "# We want steps to return messages to append to the list\n",
        "# So we annotate the messages attribute with `add_messages` reducer\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"The state of the agent.\"\"\"\n",
        "\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "\n",
        "    is_last_step: IsLastStep\n",
        "\n",
        "    remaining_steps: RemainingSteps\n",
        "\n",
        "\n",
        "class AgentStatePydantic(BaseModel):\n",
        "    \"\"\"The state of the agent.\"\"\"\n",
        "\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "\n",
        "    remaining_steps: RemainingSteps = 25\n",
        "\n",
        "\n",
        "class AgentStateWithStructuredResponse(AgentState):\n",
        "    \"\"\"The state of the agent with a structured response.\"\"\"\n",
        "\n",
        "    structured_response: StructuredResponse\n",
        "\n",
        "\n",
        "class AgentStateWithStructuredResponsePydantic(AgentStatePydantic):\n",
        "    \"\"\"The state of the agent with a structured response.\"\"\"\n",
        "\n",
        "    structured_response: StructuredResponse\n",
        "\n",
        "\n",
        "StateSchema = TypeVar(\"StateSchema\", bound=Union[AgentState, AgentStatePydantic])\n",
        "StateSchemaType = Type[StateSchema]\n",
        "\n",
        "PROMPT_RUNNABLE_NAME = \"Prompt\"\n",
        "\n",
        "Prompt = Union[\n",
        "    SystemMessage,\n",
        "    str,\n",
        "    Callable[[StateSchema], LanguageModelInput],\n",
        "    Runnable[StateSchema, LanguageModelInput],\n",
        "]\n",
        "\n",
        "\n",
        "def _get_state_value(state: StateSchema, key: str, default: Any = None) -> Any:\n",
        "    return (\n",
        "        state.get(key, default)\n",
        "        if isinstance(state, dict)\n",
        "        else getattr(state, key, default)\n",
        "    )\n",
        "\n",
        "\n",
        "def _get_prompt_runnable(prompt: Optional[Prompt]) -> Runnable:\n",
        "    prompt_runnable: Runnable\n",
        "    if prompt is None:\n",
        "        prompt_runnable = RunnableCallable(\n",
        "            lambda state: _get_state_value(state, \"messages\"), name=PROMPT_RUNNABLE_NAME\n",
        "        )\n",
        "    elif isinstance(prompt, str):\n",
        "        _system_message: BaseMessage = SystemMessage(content=prompt)\n",
        "        prompt_runnable = RunnableCallable(\n",
        "            lambda state: [_system_message] + _get_state_value(state, \"messages\"),\n",
        "            name=PROMPT_RUNNABLE_NAME,\n",
        "        )\n",
        "    elif isinstance(prompt, SystemMessage):\n",
        "        prompt_runnable = RunnableCallable(\n",
        "            lambda state: [prompt] + _get_state_value(state, \"messages\"),\n",
        "            name=PROMPT_RUNNABLE_NAME,\n",
        "        )\n",
        "    elif inspect.iscoroutinefunction(prompt):\n",
        "        prompt_runnable = RunnableCallable(\n",
        "            None,\n",
        "            prompt,\n",
        "            name=PROMPT_RUNNABLE_NAME,\n",
        "        )\n",
        "    elif callable(prompt):\n",
        "        prompt_runnable = RunnableCallable(\n",
        "            prompt,\n",
        "            name=PROMPT_RUNNABLE_NAME,\n",
        "        )\n",
        "    elif isinstance(prompt, Runnable):\n",
        "        prompt_runnable = prompt\n",
        "    else:\n",
        "        raise ValueError(f\"Got unexpected type for `prompt`: {type(prompt)}\")\n",
        "\n",
        "    return prompt_runnable\n",
        "\n",
        "\n",
        "def _convert_modifier_to_prompt(func: F) -> F:\n",
        "    \"\"\"Decorator that converts state_modifier kwarg to prompt kwarg.\"\"\"\n",
        "\n",
        "    @functools.wraps(func)\n",
        "    def wrapper(*args: Any, **kwargs: Any) -> Any:\n",
        "        prompt = kwargs.get(\"prompt\")\n",
        "        state_modifier = kwargs.pop(\"state_modifier\", None)\n",
        "        if sum(p is not None for p in (prompt, state_modifier)) > 1:\n",
        "            raise ValueError(\n",
        "                \"Expected only one of (prompt, state_modifier), got multiple values\"\n",
        "            )\n",
        "\n",
        "        if state_modifier is not None:\n",
        "            prompt = state_modifier\n",
        "\n",
        "        kwargs[\"prompt\"] = prompt\n",
        "        return func(*args, **kwargs)\n",
        "\n",
        "    return cast(F, wrapper)\n",
        "\n",
        "\n",
        "def _should_bind_tools(model: LanguageModelLike, tools: Sequence[BaseTool]) -> bool:\n",
        "    if isinstance(model, RunnableSequence):\n",
        "        model = next(\n",
        "            (\n",
        "                step\n",
        "                for step in model.steps\n",
        "                if isinstance(step, (RunnableBinding, BaseChatModel))\n",
        "            ),\n",
        "            model,\n",
        "        )\n",
        "\n",
        "    if not isinstance(model, RunnableBinding):\n",
        "        return True\n",
        "\n",
        "    if \"tools\" not in model.kwargs:\n",
        "        return True\n",
        "\n",
        "    bound_tools = model.kwargs[\"tools\"]\n",
        "    if len(tools) != len(bound_tools):\n",
        "        raise ValueError(\n",
        "            \"Number of tools in the model.bind_tools() and tools passed to create_react_agent must match\"\n",
        "        )\n",
        "\n",
        "    tool_names = set(tool.name for tool in tools)\n",
        "    bound_tool_names = set()\n",
        "    for bound_tool in bound_tools:\n",
        "        # OpenAI-style tool\n",
        "        if bound_tool.get(\"type\") == \"function\":\n",
        "            bound_tool_name = bound_tool[\"function\"][\"name\"]\n",
        "        # Anthropic-style tool\n",
        "        elif bound_tool.get(\"name\"):\n",
        "            bound_tool_name = bound_tool[\"name\"]\n",
        "        else:\n",
        "            # unknown tool type so we'll ignore it\n",
        "            continue\n",
        "\n",
        "        bound_tool_names.add(bound_tool_name)\n",
        "\n",
        "    if missing_tools := tool_names - bound_tool_names:\n",
        "        raise ValueError(f\"Missing tools '{missing_tools}' in the model.bind_tools()\")\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def _get_model(model: LanguageModelLike) -> BaseChatModel:\n",
        "    \"\"\"Get the underlying model from a RunnableBinding or return the model itself.\"\"\"\n",
        "    if isinstance(model, RunnableSequence):\n",
        "        model = next(\n",
        "            (\n",
        "                step\n",
        "                for step in model.steps\n",
        "                if isinstance(step, (RunnableBinding, BaseChatModel))\n",
        "            ),\n",
        "            model,\n",
        "        )\n",
        "\n",
        "    if isinstance(model, RunnableBinding):\n",
        "        model = model.bound\n",
        "\n",
        "    if not isinstance(model, BaseChatModel):\n",
        "        raise TypeError(\n",
        "            f\"Expected `model` to be a ChatModel or RunnableBinding (e.g. model.bind_tools(...)), got {type(model)}\"\n",
        "        )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def _validate_chat_history(\n",
        "    messages: Sequence[BaseMessage],\n",
        ") -> None:\n",
        "    \"\"\"Validate that all tool calls in AIMessages have a corresponding ToolMessage.\"\"\"\n",
        "    all_tool_calls = [\n",
        "        tool_call\n",
        "        for message in messages\n",
        "        if isinstance(message, AIMessage)\n",
        "        for tool_call in message.tool_calls\n",
        "    ]\n",
        "    tool_call_ids_with_results = {\n",
        "        message.tool_call_id for message in messages if isinstance(message, ToolMessage)\n",
        "    }\n",
        "    tool_calls_without_results = [\n",
        "        tool_call\n",
        "        for tool_call in all_tool_calls\n",
        "        if tool_call[\"id\"] not in tool_call_ids_with_results\n",
        "    ]\n",
        "    if not tool_calls_without_results:\n",
        "        return\n",
        "\n",
        "    error_message = create_error_message(\n",
        "        message=\"Found AIMessages with tool_calls that do not have a corresponding ToolMessage. \"\n",
        "        f\"Here are the first few of those tool calls: {tool_calls_without_results[:3]}.\\n\\n\"\n",
        "        \"Every tool call (LLM requesting to call a tool) in the message history MUST have a corresponding ToolMessage \"\n",
        "        \"(result of a tool invocation to return to the LLM) - this is required by most LLM providers.\",\n",
        "        error_code=ErrorCode.INVALID_CHAT_HISTORY,\n",
        "    )\n",
        "    raise ValueError(error_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtCBJ8Vs6-c0"
      },
      "source": [
        "### **Membuat Agent Organizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVcsF_iJ6-c2"
      },
      "outputs": [],
      "source": [
        "def create_agent(\n",
        "    model: Union[str, LanguageModelLike],\n",
        "    tools: Union[Sequence[Union[BaseTool, Callable]], ToolNode],\n",
        "    *,\n",
        "    prompt: Optional[Prompt] = None,\n",
        "    name: Optional[str] = None,\n",
        "):\n",
        "\n",
        "    if isinstance(tools, ToolNode):\n",
        "        tool_classes = list(tools.tools_by_name.values())\n",
        "        tool_node = tools\n",
        "    else:\n",
        "        tool_node = ToolNode(tools)\n",
        "        # get the tool functions wrapped in a tool class from the ToolNode\n",
        "        tool_classes = list(tool_node.tools_by_name.values())\n",
        "\n",
        "    if isinstance(model, str):\n",
        "        try:\n",
        "            from langchain.chat_models import (  # type: ignore[import-not-found]\n",
        "                init_chat_model,\n",
        "            )\n",
        "        except ImportError:\n",
        "            raise ImportError(\n",
        "                \"Please install langchain (`pip install langchain`) to use '<provider>:<model>' string syntax for `model` parameter.\"\n",
        "            )\n",
        "\n",
        "        model = cast(BaseChatModel, init_chat_model(model))\n",
        "\n",
        "    tool_calling_enabled = len(tool_classes) > 0\n",
        "\n",
        "    if _should_bind_tools(model, tool_classes) and tool_calling_enabled:\n",
        "        model = cast(BaseChatModel, model).bind_tools(tool_classes)\n",
        "\n",
        "    model_runnable = _get_prompt_runnable(prompt) | model\n",
        "\n",
        "    def _are_more_steps_needed(state: StateSchema, response: BaseMessage) -> bool:\n",
        "        has_tool_calls = isinstance(response, AIMessage) and response.tool_calls\n",
        "        remaining_steps = _get_state_value(state, \"remaining_steps\", None)\n",
        "        is_last_step = _get_state_value(state, \"is_last_step\", False)\n",
        "        return (\n",
        "            (remaining_steps is None and is_last_step and has_tool_calls)\n",
        "            or (remaining_steps is not None and remaining_steps < 2 and has_tool_calls)\n",
        "        )\n",
        "\n",
        "    # Define the function that calls the model\n",
        "    def call_model(state: StateSchema, config: RunnableConfig) -> StateSchema:\n",
        "        messages = _get_state_value(state, \"messages\")\n",
        "        _validate_chat_history(messages)\n",
        "        response = cast(AIMessage, model_runnable.invoke(state, config))\n",
        "        # add agent name to the AIMessage\n",
        "        response.name = name\n",
        "\n",
        "        if _are_more_steps_needed(state, response):\n",
        "            return {\n",
        "                \"messages\": [\n",
        "                    AIMessage(\n",
        "                        id=response.id,\n",
        "                        content=\"Sorry, need more steps to process this request.\",\n",
        "                    )\n",
        "                ]\n",
        "            }\n",
        "        # We return a list, because this will get added to the existing list\n",
        "        return {\"messages\": [response]}\n",
        "\n",
        "    async def acall_model(state: StateSchema, config: RunnableConfig) -> StateSchema:\n",
        "        messages = _get_state_value(state, \"messages\")\n",
        "        _validate_chat_history(messages)\n",
        "        response = cast(AIMessage, await model_runnable.ainvoke(state, config))\n",
        "        # add agent name to the AIMessage\n",
        "        response.name = name\n",
        "        if _are_more_steps_needed(state, response):\n",
        "            return {\n",
        "                \"messages\": [\n",
        "                    AIMessage(\n",
        "                        id=response.id,\n",
        "                        content=\"Sorry, need more steps to process this request.\",\n",
        "                    )\n",
        "                ]\n",
        "            }\n",
        "        # We return a list, because this will get added to the existing list\n",
        "        return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "    return RunnableCallable(call_model, acall_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7BOSxqH6-c3"
      },
      "source": [
        "### **Membuat Agent Perangkum**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdJ3xTYj6-c3"
      },
      "outputs": [],
      "source": [
        "# Bisa aku modifikasi sehingga menjadi Node Perangkum\n",
        "\n",
        "# def generate_structured_response(\n",
        "#     state: StateSchema, config: RunnableConfig\n",
        "# ) -> StateSchema:\n",
        "#     # NOTE: we exclude the last message because there is enough information\n",
        "#     # for the LLM to generate the structured response\n",
        "#     messages = _get_state_value(state, \"messages\")[:-1]\n",
        "#     structured_response_schema = response_format\n",
        "#     if isinstance(response_format, tuple):\n",
        "#         system_prompt, structured_response_schema = response_format\n",
        "#         messages = [SystemMessage(content=system_prompt)] + list(messages)\n",
        "\n",
        "#     model_with_structured_output = _get_model(model).with_structured_output(\n",
        "#         cast(StructuredResponseSchema, structured_response_schema)\n",
        "#     )\n",
        "#     response = model_with_structured_output.invoke(messages, config)\n",
        "#     return {\"structured_response\": response}\n",
        "\n",
        "# async def agenerate_structured_response(\n",
        "#     state: StateSchema, config: RunnableConfig\n",
        "# ) -> StateSchema:\n",
        "#     # NOTE: we exclude the last message because there is enough information\n",
        "#     # for the LLM to generate the structured response\n",
        "#     messages = _get_state_value(state, \"messages\")[:-1]\n",
        "#     structured_response_schema = response_format\n",
        "#     if isinstance(response_format, tuple):\n",
        "#         system_prompt, structured_response_schema = response_format\n",
        "#         messages = [SystemMessage(content=system_prompt)] + list(messages)\n",
        "\n",
        "#     model_with_structured_output = _get_model(model).with_structured_output(\n",
        "#         cast(StructuredResponseSchema, structured_response_schema)\n",
        "#     )\n",
        "#     response = await model_with_structured_output.ainvoke(messages, config)\n",
        "#     return {\"structured_response\": response}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKBXBuGn6-c4"
      },
      "source": [
        "### **Membuat Worflow**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr-zg3SA6-c4"
      },
      "outputs": [],
      "source": [
        "def create_workflow(\n",
        "    model: Union[str, LanguageModelLike],\n",
        "    tools: Union[Sequence[Union[BaseTool, Callable]], ToolNode],\n",
        "    *,\n",
        "    prompt: Optional[Prompt] = None,\n",
        "    state_schema: Optional[StateSchemaType] = None,\n",
        "    config_schema: Optional[Type[Any]] = None,\n",
        "    checkpointer: Optional[Checkpointer] = None,\n",
        "    name: Optional[str] = None,\n",
        "):\n",
        "    if state_schema is not None:\n",
        "        required_keys = {\"messages\", \"remaining_steps\"}\n",
        "\n",
        "        schema_keys = set(get_type_hints(state_schema))\n",
        "        if missing_keys := required_keys - set(schema_keys):\n",
        "            raise ValueError(f\"Missing required key(s) {missing_keys} in state_schema\")\n",
        "\n",
        "    if state_schema is None:\n",
        "        state_schema = (AgentState)\n",
        "\n",
        "    if isinstance(tools, ToolNode):\n",
        "        tool_node = tools\n",
        "    else:\n",
        "        tool_node = ToolNode(tools)\n",
        "\n",
        "    # JIKA not last_message.tool_calls MAKA ARAHKAN KE NODE PERANGKUM\n",
        "    # Define the function that determines whether to continue or not\n",
        "    def should_continue(state: StateSchema) -> Union[str, list]:\n",
        "        messages = _get_state_value(state, \"messages\")\n",
        "        last_message = messages[-1]\n",
        "        # If there is no function call, then we finish\n",
        "        if not isinstance(last_message, AIMessage) or not last_message.tool_calls:\n",
        "            return END if response_format is None else \"generate_structured_response\"\n",
        "        # Otherwise if there is, we continue\n",
        "        else:\n",
        "                tool_calls = [\n",
        "                    tool_node.inject_tool_args(call, state)  # type: ignore[arg-type]\n",
        "                    for call in last_message.tool_calls\n",
        "                ]\n",
        "                return [Send(\"tools\", [tool_call]) for tool_call in tool_calls]\n",
        "\n",
        "    # MEMBUAT WORKFLOW\n",
        "    #########################################################################################\n",
        "\n",
        "    # Define a new graph\n",
        "    workflow = StateGraph(state_schema or AgentState, config_schema=config_schema)\n",
        "\n",
        "    # Define the two nodes we will cycle between\n",
        "    workflow.add_node(\"agent\", create_agent(model, tool_node, prompt=prompt, name=\"agent_organizer\"))\n",
        "    workflow.add_node(\"tools\", tool_node)\n",
        "\n",
        "    # Set the entrypoint as `agent`\n",
        "    # This means that this node is the first one called\n",
        "    workflow.set_entry_point(\"agent\")\n",
        "\n",
        "    # UBAH generate_structured_response MENJADI NODE PERANGKUM\n",
        "    # Add a structured output node if response_format is provided\n",
        "    workflow.add_node(\n",
        "        \"generate_structured_response\",\n",
        "        RunnableCallable(\n",
        "            generate_structured_response, agenerate_structured_response\n",
        "        ),\n",
        "    )\n",
        "    workflow.add_edge(\"generate_structured_response\", END)\n",
        "    should_continue_destinations = [\"tools\", \"generate_structured_response\"]\n",
        "\n",
        "    # We now add a conditional edge\n",
        "    workflow.add_conditional_edges(\n",
        "        # First, we define the start node. We use `agent`.\n",
        "        # This means these are the edges taken after the `agent` node is called.\n",
        "        \"agent\",\n",
        "        # Next, we pass in the function that will determine which node is called next.\n",
        "        should_continue,\n",
        "        path_map=should_continue_destinations,\n",
        "    )\n",
        "\n",
        "    workflow.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "    # Finally, we compile it!\n",
        "    # This compiles it into a LangChain Runnable,\n",
        "    # meaning you can use it as you would any other runnable\n",
        "    return workflow.compile(\n",
        "        checkpointer=checkpointer,\n",
        "        name=name\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
