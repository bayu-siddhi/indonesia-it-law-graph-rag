{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "# import dateparser\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.remote.webdriver import WebDriver\n",
    "# from selenium.webdriver.remote.webelement import WebElement\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPKScraper:\n",
    "\n",
    "    def __init__(self, web_driver: WebDriver):\n",
    "        self.web_driver = web_driver\n",
    "        self.ENCODE = {\n",
    "            'type': {\n",
    "                'UU': '01',\n",
    "                'PERPPU': '02',\n",
    "                'PP': '03',\n",
    "                'PERPRES': '04',\n",
    "                'PERMENKOMINFO': '05'\n",
    "            },\n",
    "            'section': {\n",
    "                'document': '1',\n",
    "                'considering': '2',\n",
    "                'observing': '3',\n",
    "                'definition': '4',\n",
    "                'chapter': '5',\n",
    "                'article': '6',\n",
    "                'section': '7',\n",
    "            }\n",
    "        }\n",
    "    \n",
    "\n",
    "    @staticmethod  # https://stackoverflow.com/questions/735975/static-methods-in-python\n",
    "    def list_of_dict_to_json(regulation_data: list[dict], output_path: str) -> None:\n",
    "        if not output_path.endswith('.json'):\n",
    "            output_path = output_path + '.json'\n",
    "        with open(output_path, 'w', encoding='utf-8') as file:\n",
    "            json.dump(regulation_data, file, indent=4)\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def list_of_dict_to_excel(regulation_data: list[dict], output_path: str) -> None:\n",
    "        if not output_path.endswith('.xlsx'):\n",
    "            output_path = output_path + '.xlsx'\n",
    "        df = pd.DataFrame(regulation_data)\n",
    "        df.to_excel(output_path, index=False)\n",
    "\n",
    "\n",
    "    def active_regulation(self, url: str, regulation_type: str, verbose: bool = True) -> list[dict]:\n",
    "        regulations_box_xpath = '/html/body/div/div/div[2]/div[2]/div[2]'\n",
    "        regulations_css_selector = 'div.row.mb-8[class=\"row mb-8\"]'\n",
    "        regulation_href_css_selector = 'div.col-lg-10.fs-2.fw-bold.pe-4 a'\n",
    "        pagination_box_css_selector = 'ul.pagination.justify-content-center'\n",
    "        regulation_number_css_selector = 'div.col-lg-8.fw-semibold.fs-5.text-gray-600'\n",
    "        regulation_title_css_selector = 'div.col-lg-10.fs-2.fw-bold.pe-4'\n",
    "        regulation_subjects_css_selector = 'span.badge.badge-light-primary.mb-2'\n",
    "        page_pattern = r'p=(\\d+)'\n",
    "        \n",
    "        # Final result\n",
    "        active_regulations = list()\n",
    "        durations = list()\n",
    "        new_url = ''\n",
    "\n",
    "        # Check page numbering in URL\n",
    "        if re.search(page_pattern, url):\n",
    "            new_url = re.sub(page_pattern, 'p={page}', url)\n",
    "        else:\n",
    "            new_url = url + '&p={page}'\n",
    "\n",
    "        # Get last page number\n",
    "        self.web_driver.get(new_url.format(page=1))\n",
    "        wait = WebDriverWait(self.web_driver, timeout=10)\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, pagination_box_css_selector)))\n",
    "        \n",
    "        pagination_box = self.web_driver.find_element(By.CSS_SELECTOR, pagination_box_css_selector)\n",
    "        last_page_button = pagination_box.find_element(By.XPATH, \"./*[last()]\")\n",
    "        last_page_href = last_page_button.find_element(By.CSS_SELECTOR, '[href]').get_attribute('href')\n",
    "        last_page_number = int(re.search(page_pattern, last_page_href)[1])\n",
    "\n",
    "        # Iterate for every page\n",
    "        for page in tqdm(iterable=range(1, last_page_number + 1), desc='Scraping active regulations', disable=not verbose):\n",
    "            start = time.time()\n",
    "            \n",
    "            # Go to the page\n",
    "            access_page = False\n",
    "            trial_number = 10\n",
    "\n",
    "            # TODO: Apply trial access ini ke semua akses web di file lain\n",
    "            for _ in range(trial_number):\n",
    "                try:\n",
    "                    # Try access the page\n",
    "                    self.web_driver.get(new_url.format(page=page))\n",
    "                    wait = WebDriverWait(self.web_driver, timeout=10)\n",
    "                    wait.until(EC.presence_of_element_located((By.XPATH, regulations_box_xpath)))\n",
    "                    access_page = True\n",
    "                    break\n",
    "                except TimeoutException as e:\n",
    "                    # If timeout, wait for 2 seconds\n",
    "                    time.sleep(2)\n",
    "            \n",
    "            if not access_page:\n",
    "                if verbose:\n",
    "                    print(f'Unable to access {url} on page={page} after {trial_number} attempts')\n",
    "                    print(f'Skip the scraping process to page={page + 1}')\n",
    "                continue\n",
    "            \n",
    "            # Get all regulation instance\n",
    "            regulations_box = self.web_driver.find_element(By.XPATH, regulations_box_xpath)\n",
    "            regulations_all = regulations_box.find_elements(By.CSS_SELECTOR, regulations_css_selector)\n",
    "\n",
    "            # Iterate for every regulation\n",
    "            for regulation in regulations_all:\n",
    "                # Ignore all ineffective regulations\n",
    "                if not re.findall(r'Dicabut dengan', regulation.text):\n",
    "                    \n",
    "                    # Get regulation number and year element\n",
    "                    regulation_number_and_year = regulation.find_element(By.CSS_SELECTOR, regulation_number_css_selector).text.lower()\n",
    "                    \n",
    "                    # Get regulation number\n",
    "                    new_regulation_number = re.search(r'\\b(?:nomor|no\\.)\\s+(\\d+)', regulation_number_and_year)\n",
    "                    old_regulation_number = re.search(r'(\\d+)\\/', new_regulation_number[0])\n",
    "                    regulation_number = new_regulation_number[1] if old_regulation_number is None else old_regulation_number[1]\n",
    "\n",
    "                    # Get regulation year\n",
    "                    regulation_year = re.search(r'tahun\\s+(\\d+)', regulation_number_and_year)\n",
    "                    regulation_year = regulation_year[1] if regulation_year is not None else ''\n",
    "\n",
    "                    # Get regulation title\n",
    "                    regulation_title = regulation.find_element(By.CSS_SELECTOR, regulation_title_css_selector).text.strip()\n",
    "                    \n",
    "                    # Get regulation subjects\n",
    "                    regulation_subjects = list()\n",
    "                    regulation_subject_elements = regulation.find_elements(By.CSS_SELECTOR, regulation_subjects_css_selector)\n",
    "                    if regulation_subject_elements:\n",
    "                        for subject in regulation_subject_elements:\n",
    "                            regulation_subjects.append(subject.text)\n",
    "                    \n",
    "                    # Get regulation URL link\n",
    "                    regulation_href = regulation.find_element(By.CSS_SELECTOR, regulation_href_css_selector).get_attribute(\"href\")\n",
    "                    \n",
    "                    # Create regulation temporary ID, just for ordering\n",
    "                    # number = f'{regulation_year}{regulation_number.zfill(3)}'\n",
    "                    regulation_id = f'{regulation_type}_{regulation_year}_{regulation_number.zfill(3)}'\n",
    "\n",
    "                    # Append all data\n",
    "                    active_regulations.append({\n",
    "                        # 'no': number,\n",
    "                        'name': regulation_id,\n",
    "                        'about': regulation_title,\n",
    "                        'subjects': regulation_subjects,\n",
    "                        'url_1': regulation_href,\n",
    "                        'url_2': '',\n",
    "                        'good_pdf': False,\n",
    "                        'used': False,\n",
    "                        'notes': ''\n",
    "                    })\n",
    "            \n",
    "            durations.append(time.time() - start)\n",
    "            time.sleep(2)  # Break for 2 seconds\n",
    "        \n",
    "        self.web_driver.quit()\n",
    "\n",
    "        if verbose:\n",
    "            print('=' * 76)\n",
    "            print(f'URL               : {url}')\n",
    "            print(f'Regulation type   : {regulation_type}')\n",
    "            print(f'Total regulations : {len(active_regulations)} regulations')\n",
    "            print(f'Total time        : {round(sum(durations), 3)} seconds')\n",
    "            print(f'Average time      : {round(sum(durations) / len(active_regulations), 3)} seconds')\n",
    "            print('NOTE! Time records do not include the 2 seconds break between each regulation')\n",
    "            print('=' * 76)\n",
    "\n",
    "        return active_regulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change These Input Data\n",
    "input_data = [\n",
    "    {\n",
    "        'regulation_type': 'UU',\n",
    "        'url': 'https://peraturan.bpk.go.id/Search?keywords=&tentang=&nomor=&jenis=8&tema=55'\n",
    "    },\n",
    "    {\n",
    "        'regulation_type': 'PP',\n",
    "        'url': 'https://peraturan.bpk.go.id/Search?keywords=&tentang=&nomor=&jenis=10&tema=55'\n",
    "    },\n",
    "    {\n",
    "        'regulation_type': 'PERMENKOMINFO',\n",
    "        'url': 'https://peraturan.bpk.go.id/Search?keywords=&tentang=&nomor=&entitas=603&tema=55'\n",
    "    }\n",
    "]\n",
    "\n",
    "DIR_PATH = os.path.join('data', 'active')\n",
    "os.makedirs(DIR_PATH, exist_ok=True)\n",
    "# all_active_regulations = list()\n",
    "\n",
    "for data in input_data:\n",
    "    # Scrape active regulation links\n",
    "    web_driver = webdriver.Firefox()\n",
    "    scraper = BPKScraper(web_driver=web_driver)\n",
    "    active_regulations = scraper.active_regulation(\n",
    "        url=data['url'],\n",
    "        regulation_type=data['regulation_type'],\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Save output to .json file\n",
    "    output_path = os.path.join(DIR_PATH, f'{data[\"regulation_type\"]}.json')\n",
    "    BPKScraper.list_of_dict_to_json(\n",
    "        regulation_data=active_regulations,\n",
    "        output_path=output_path\n",
    "    )\n",
    "\n",
    "    # Save output to .XSLX file\n",
    "    output_path = os.path.join(DIR_PATH, f'{data[\"regulation_type\"]}.xlsx')\n",
    "    BPKScraper.list_of_dict_to_excel(\n",
    "        regulation_data=active_regulations,\n",
    "        output_path=output_path\n",
    "    )\n",
    "\n",
    "    # all_active_regulations = all_active_regulations + active_regulations\n",
    "\n",
    "# # Save all output to .json file\n",
    "# output_path = os.path.join(DIR_PATH, 'active_regulations.json')\n",
    "# BPKScraper.list_of_dict_to_json(\n",
    "#     regulation_data=active_regulations,\n",
    "#     output_path=output_path\n",
    "# )\n",
    "\n",
    "# # Save all output to .XSLX file\n",
    "# output_path = os.path.join(DIR_PATH, 'active_regulations.xlsx')\n",
    "# BPKScraper.list_of_dict_to_excel(\n",
    "#     regulation_data=active_regulations,\n",
    "#     output_path=output_path\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scrape_active_regulation_links(url: str, keyword: str) -> list[str]:\n",
    "#     regulations_box_xpath = '/html/body/div/div/div[2]/div[2]/div[2]'\n",
    "#     regulations_css_selector = 'div.row.mb-8[class=\"row mb-8\"]'\n",
    "#     regulation_href_css_selector = 'div.col-lg-10.fs-2.fw-bold.pe-4 a'\n",
    "#     pagination_box_css_selector = 'ul.pagination.justify-content-center'\n",
    "#     regulation_number_css_selector = 'div.col-lg-8.fw-semibold.fs-5.text-gray-600'\n",
    "#     regulation_title_css_selector = 'div.col-lg-10.fs-2.fw-bold.pe-4'\n",
    "#     regulation_subjects_css_selector = 'span.badge.badge-light-primary.mb-2'\n",
    "#     page_pattern = r'p=(\\d+)'\n",
    "\n",
    "#     driver = webdriver.Firefox()\n",
    "#     active_regulations_list = list()\n",
    "\n",
    "#     # CHECK PAGE NUMBERING IN URL\n",
    "#     if re.search(page_pattern, url):\n",
    "#         url = re.sub(page_pattern, 'p={page}', url)\n",
    "#     else:\n",
    "#         url = url + '&p={page}'\n",
    "\n",
    "#     # GET LAST PAGE NUMBER\n",
    "#     driver.get(url.format(page=1))\n",
    "#     wait = WebDriverWait(driver, timeout=5)\n",
    "#     wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, pagination_box_css_selector)))\n",
    "#     pagination_box = driver.find_element(By.CSS_SELECTOR, pagination_box_css_selector)\n",
    "#     last_page_button = pagination_box.find_element(By.XPATH, \"./*[last()]\")\n",
    "#     last_page_href = last_page_button.find_element(By.CSS_SELECTOR, '[href]').get_attribute('href')\n",
    "#     last_page_number = int(re.search(page_pattern, last_page_href)[1])\n",
    "\n",
    "#     # ITERATE FOR ALL PAGE\n",
    "#     for page in tqdm(range(1, last_page_number + 1)):\n",
    "#         driver.get(url.format(page=page))\n",
    "#         wait = WebDriverWait(driver, timeout=5)\n",
    "#         wait.until(EC.presence_of_element_located((By.XPATH, regulations_box_xpath)))\n",
    "        \n",
    "#         regulations_box = driver.find_element(By.XPATH, regulations_box_xpath)\n",
    "#         regulations_all = regulations_box.find_elements(By.CSS_SELECTOR, regulations_css_selector)\n",
    "\n",
    "#         for regulation in regulations_all:\n",
    "#             if not re.findall('Dicabut dengan', regulation.text):\n",
    "                \n",
    "#                 # Regulation Number & Year\n",
    "#                 regulation_number_and_year = regulation.find_element(By.CSS_SELECTOR, regulation_number_css_selector).text.lower()\n",
    "                \n",
    "#                 # Get Regulation Number\n",
    "#                 new_regulation_number = re.search(r'\\b(?:nomor|no\\.)\\s+(\\d+)', regulation_number_and_year)\n",
    "#                 old_regulation_number = re.search(r'(\\d+)\\/', new_regulation_number[0])\n",
    "#                 regulation_number = new_regulation_number[1] if old_regulation_number is None else old_regulation_number[1]\n",
    "\n",
    "#                 # Get Regulation Year\n",
    "#                 regulation_year = re.search(r'tahun\\s+(\\d+)', regulation_number_and_year)\n",
    "#                 regulation_year = regulation_year[1] if regulation_year is not None else ''\n",
    "\n",
    "#                 # Get Regulation Title\n",
    "#                 regulation_title = regulation.find_element(By.CSS_SELECTOR, regulation_title_css_selector).text.strip()\n",
    "                \n",
    "#                 # Get Regulation Subjects\n",
    "#                 regulation_subjects = list()\n",
    "#                 regulation_subject_elements = regulation.find_elements(By.CSS_SELECTOR, regulation_subjects_css_selector)\n",
    "#                 if regulation_subject_elements:\n",
    "#                     for subject in regulation_subject_elements:\n",
    "#                         regulation_subjects.append(subject.text)\n",
    "                \n",
    "#                 # Get Regulation URL Link\n",
    "#                 regulation_href = regulation.find_element(By.CSS_SELECTOR, regulation_href_css_selector).get_attribute(\"href\")\n",
    "                \n",
    "#                 # Build Regulation ID\n",
    "#                 number = f'{regulation_year}{regulation_number.zfill(3)}'\n",
    "#                 regulation_id = f'{keyword}_{regulation_number.zfill(3)}_{regulation_year}'\n",
    "\n",
    "#                 # Store All Data\n",
    "#                 active_regulations_list.append({\n",
    "#                     'no': number,\n",
    "#                     'name': regulation_id,\n",
    "#                     'about': regulation_title,\n",
    "#                     'subjects': regulation_subjects,\n",
    "#                     'url': regulation_href,\n",
    "#                     'used': False\n",
    "#                 })\n",
    "        \n",
    "#         time.sleep(1)\n",
    "    \n",
    "#     driver.quit()\n",
    "#     return active_regulations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Change These Input Data\n",
    "# # keyword = 'PERMENKOMINFO'\n",
    "# # url = 'https://peraturan.bpk.go.id/Search?keywords=&tentang=&nomor=&entitas=603&tema=55'\n",
    "\n",
    "# # keyword = 'UU'\n",
    "# # url = 'https://peraturan.bpk.go.id/Search?keywords=&tentang=&nomor=&jenis=8&tema=55'\n",
    "\n",
    "# keyword = 'PP'\n",
    "# url = 'https://peraturan.bpk.go.id/Search?keywords=&tentang=&nomor=&jenis=10&tema=55'\n",
    "\n",
    "# # Scrape Active Regulation Links\n",
    "# active_regulations_list = scrape_active_regulation_links(url, keyword)\n",
    "# print(f'Successfully scraping {len(active_regulations_list)} regulatory links')\n",
    "\n",
    "# # Save Output to .XSLX File\n",
    "# base_directory = os.path.join('output', 'active')\n",
    "# os.makedirs(base_directory, exist_ok=True)\n",
    "\n",
    "# output_path = os.path.join(base_directory, f'{keyword}.xlsx')\n",
    "# df = pd.DataFrame(active_regulations_list)\n",
    "# df.to_excel(output_path, index=False)\n",
    "\n",
    "# print(f'Successfully saved {len(active_regulations_list)} regulatory links to {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_regulations_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
